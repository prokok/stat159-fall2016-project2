```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(xtable)
```

#Analysis

 In analysis part, we will see the results of the each methods we applied to a given data set.

###1) Ordinary Least Square Method(OLS)  

```{r echo = FALSE}
load("../../data/ols-regression.RData")
```

  There are several steps in applying the Ordinary Least Square method.
  
  step1. Using the result from F-statistics, ruling out the qualitative variables that is not significant in identifying the response variable, balance.

```{r echo = FALSE, results='asis'}
load( "../../data/anova-qual.RData")
x = xtable(anova_ethnicity, digits=2,"Anova analysis of ethnicity & Balance")
print(x, comment=FALSE, type="html")
x = xtable(anova_gender, digits=2,"Anova analysis of gender & Balance")
print(x, comment=FALSE, type="html")
x = xtable(anova_marriage, digits=2,"Anova analysis of marriage & Balance")
print(x, comment=FALSE, type="html")
x = xtable(anova_student, digits=2,"Anova analysis of student & Balance")
print(x, comment=FALSE, type="html")
```
  
  As you can see in the anova tables, only student variable is signifant in evaluating the response variable balance. Thus, we are ruling out ethnicity, gender, and marrage variables.
  
  step2. Apply the OLS on our training dataset on selected predictor variables: income, limit, rating, cards, ages, eduction, student
  
```{r echo = FALSE, results='asis'}
x = xtable(reg2, digits=4, "MODEL1 using selected predictor variables: income, limit, rating, cards, ages, education, student")
print(x, commnet=FALSE, type='html')
```

  This also enables us to identify the optimal predictors to predict the balance: income, limit, cards, students.
  
  Step3. Using the result from step2, apply linear regression model on opitmal predictor variables to estimate the balance: income, limit, cards, student.

```{r echo = FALSE, results='asis'}
x = xtable(reg3, digits=4, "MODEL2 using Optimal predictor variables: income, limitm cards, student")
print(x, commnet=FALSE, type='html')

```

  Step4. Identify the best model.
  
```{r echo = FALSE, results='asis'}
  k = data.frame(Adjusted.R.Square= c("Model1","Model2"), Value = c(summary(reg2)$adj.r.squared,summary(reg3)$adj.r.squared))
  x=xtable(k,digits=4, captions = "Adjusted R Square of Model1 and Model2")
  print(x, comment=FALSE, type = 'html')

```
  
  To identify the best OLS model. we are going to compare the Adjusted R square of both linear models. Model1, using selected predictor yields the `r round(summary(reg2)$adj.r.squared,5)`. Model2 using optimal predictor yields `r round(summary(reg3)$adj.r.squared,5)`. Therefore, the second model using optimal predictors is the best OLS model because it uses few variables but produces almost same Adjusted R square value as the first model. 
  

###2) Ridge Regression(RR)

  
```{r echo = FALSE}
load("../../data/ridge-regression.RData")
```

  
  First, using ten-fold cross-validation, Ridge Regression selects the best model when $\lambda\ =\$ `r par_ridge` 

#2. Saving list of models(saved in Rdata file at the end)
list_models_ridge = cv.out


#3. To select the best model:
bestlambda = cv.out$lambda.min


#4. Plot the cross-validation errors in terms of the tunning 
##  parameter to visualize which parameter gives the "best" model:
png(filename = "../../images/ridge-cross-validation-errors.png", width=800, height=600)
plot(cv.out)
dev.off()


#5. Once you identify the "best" model, use the test set to compute the test Mean Square Error(test MSE)
ridge.pred = predict(cv.out, s=bestlambda, newx =x_test)
test_mse_ridge = mean(( ridge.pred -y_test)^2)


#6. Last but not least, refit the model on the full data set using the parameter chosen by cross-validation.
##  This fit will give you the "official" coefficient estimates.
ridge_full = glmnet(x_full, y_full, alpha = 0, lambda = bestlambda
                    , intercept = FALSE, standardize = FALSE)
cof_ridge = predict(ridge_full, type = "coefficients", s = bestlambda)[1:length(scaled_credit),]


#full prediction
ridge.pred_full = predict(cv.out, s=bestlambda, newx = x_full)
full_mse_ridge = mean((ridge.pred_full - y_full)^2)


#saving the the best tunning parameter, list_models_ridge, test_mse_ridge, cof_ridge, full_mse_ridge
par_ridge = bestlambda
save(par_ridge,list_models_ridge, test_mse_ridge, cof_ridge, full_mse_ridge
     , file = "../../data/ridge-regression.Rdata")

#putting the result in the eda-output.txt
sink(file = "../../data/eda-output.txt", append = TRUE)
cat("\n\n")
cat("<<Summary Statistics of Ridge-regression>>\n\n")
cat("1. Test Mean Square Error\n")
test_mse_ridge
cat("\n")
cat("2. Best Model\n")
cat("lambda equals ", par_ridge, "\n")
cat("\n")
cat("3. Official Coefficient\n")
cof_ridge
cat("\n")
sink()
  
  
  
  
  Equations above are two different representations of Ridge Regression. The first equation explains the similarity between OLS regression and Ridge Regression. Ridge regression calculates the coefficients by minimizing $RSS + \lambda\sum_{j=1}^{p}\beta_{j}^2$. If $\lambda$, the tunning parameter, is equal to zero, the estimated coefficients will be same as those of OLS regression. 
  
  The advantage of Ridge regression over OLS regression is bias-variance trade-off. As $\lambda$ increases, the estimated coefficients forced to be close to zero, leading bias estimators and decreasing the variance. As $\lambda$ decreases, the estimated coefficients do not have to be close to zero, leading less - bias estimators and increasing the variance. 




###3) Lasso Regression(LR)

$$\sum_{i=1}^{n} (y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2 + \lambda\sum_{j=1}^{p}\left|\beta_{j}\right| = RSS + \lambda\sum_{j=1}^{p}\left|\beta_{j}\right|$$

$$minimize_{\beta} \{\sum_{i=1}^{n} (y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2\}\quad subject\  to\quad \lambda\sum_{j=1}^{p}\left|\beta_{j}\right| \leq s$$

$$where\ every\ value\ of\ \lambda\ there\ is\ a\ corresponding\ s$$

  Equations above are two different representations of Lasso Regression. The first equation explains the similarity between OLS regression, Ridge regression and Lasso regression. Lasso regression calculates the coefficients by minimizing $RSS + \lambda\sum_{j=1}^{p}\left|\beta_{j}\right|$. If $\lambda$, the tunning parameter, is equal to zero, the estimated coefficients will be same as those of OLS regression. 
  
 Lasso regression has the same advantage over OLS regression as Ridge regression does: bias-variance trade-off. However, as $\lambda$ increases, the Lasso regression forces esimates to be exactly equal to zero. Thus, Lasso shrinkage method performs the variable selection and a model generated by Lasso regression is easier to interpret than a model generated by Ridge regression. Lasso regression involves only a subset of the variables referred as sparse-model.
 
 
## Dimension Reduction Methods

$$\\Z_{m} = \sum_{j=1}^{p}\phi_{jm}X_{j}$$
$$where\quad \sum_{j=1}^{p}\phi_{jm}^2\ =\ 1 $$
$$\\y_{i} = \theta_{0} + \sum_{m=1}^{M}\theta_{m}z_{im} + \varepsilon_{i} \quad i\ = \ 1, ..., n$$
$$\sum_{m=1}^{M}\theta_{m}z_{im}\ =\ \sum_{m=1}^{M}\theta_{m}\sum_{j=1}^{p}\phi_{jm}x_{ij}\ =\ \sum_{j=1}^{p}\sum_{m=1}^{M}\theta_{m}\phi_{jm}x_{ij}\ =\ \sum_{j=1}^{p}\beta_{j}x_{ij}$$ 
$$where\quad \beta_{j}\  =\ \sum_{m=1}^{M}\theta_{m}\phi_{jm}$$

  Dimenstion reduction method is transforming the predictors,$\\Z_{m} = \sum_{j=1}^{p}\phi_{jm}X_{j}$ and fit a least square models using transformed variables, $\\y_{i} = \theta_{0} + \sum_{m=1}^{M}\theta_{m}z_{im} + \varepsilon_{i} \quad i\ = \ 1, ..., n$. This approach is used when there is a large number of predictor variables. By computing M different linear combinations, transforming the predictors, we can project p predictors into a M-dimensional subspace where M is less than or equal to p. Then M projections can be used as predictors to fit a linear model by ordinary least squares.
  
  If M is equal to p, there is no dimention reduction occurs, yielding the same result equivalent to performing ordinary least squares on the p predictors.
  
  There are two major dimension reduction methods: principal components regression and partial least squares regression. Those methods work in two steps. First step is transforming the predictors, $\\Z_{m} = \sum_{j=1}^{p}\phi_{jm}X_{j}$. Second steop is selecting $\phi_{jm}$.

###4) Principle Components Regression(PCR)

  Principle Components Regression is based on the principal component analysis. Principal components analysis extracts the most variation of datasets to reduce the dimension of the data.  
  
  Find the first principal component $Z_{1}$
  $$Z_{1}\ =\ \phi_{11}X_{1} + \phi_{21}X_{2} + ... + \phi_{p1}X_{p}$$
  $Z_{1}$ is the first factor maximizing the dispersion of obervations. 
  
  Then find the second principal component $Z_{2}$
  $$Z_{2}\ =\ \phi_{12}X_{1} + \phi_{22}X_{2} + ... + \phi_{p2}X_{p}$$
  $Z_{2}$ is the second factor maximizing the dispersion of observations as well as orthogonal to the first factor.
  
  By doing this process iteratively up to desired M, it gives us the Principal Components Regression on a given M space.
  
  Principle Components Regression shows a linear regression between the data on the factors that are correlated. It enables us to solve the colinearity between preditor variables and dimensionality of the given data. That is, the number of samples, n, does not necessarily bigger than the number of predictor variables.

###5) Partial Least Squares Regression(PLSR)

  Like Principle Componenet Regression, Partial Least Square Regression calculuates $Z_{1}, .. ,Z_{m}$ and fit a linear model on those transformed predictors.However,Partial Least Square Regression takes into account the structure of both the explanatory and dependent variable unlike Principle Component Regression. That being said, it uses response variable Y to find directions that help explaining both the reponse and the predictors.  

   Find the first direction $Z_{1}$
   $$Z_{1}\ =\ \phi_{11}X_{1} + \phi_{21}X_{2} + ... + \phi_{p1}X_{p}$$  
   
   $$by\ setting\ \phi_{j1} \ =\ \beta_{j}\quad where\ \beta{j}\ is\ from\ simple\ linear\ regression\ Y\ onto\ X_{j}\ and\ j = 1,...,p$$
   

  The first direction $Z_{1}$ consequently places the weights on each predictor variables proportional to its correlation with the resoponse variable.
  
  To find the second direction $Z_{2}$, we need to regressing each variable on $Z_{1}$ and take the residuals. Then compute the $Z_{2}$ with the residuals that are orthogonal to the original data by applying the same procedure to compute the first directon $Z_{1}$
  
  By doing this process iteratively up to desired M, it gives us the Partial Least Squares Regression on a given M space.
  