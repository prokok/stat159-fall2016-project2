---
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(xtable)
```

#Analysis

 In analysis part, we will see the results of the each methods we applied to a given data set.

###1) Ordinary Least Square Method(OLS)  

```{r echo = FALSE}
load("../../data/ols-regression.RData")
```

  There are several steps in applying the Ordinary Least Square method.
  
  STEP1. Using the result from F-statistics, ruling out the qualitative variables that is not significant in identifying the response variable. As you can see in the anova Tables 1-4, only student variable is signifant in evaluating the response variable balance. Thus, we are ruling out ethnicity, gender, and marrage variables.
  
  STEP2. Apply the OLS on our training dataset on selected predictor variables: income, limit, rating, cards, ages, eduction, student as shown in Table 5. This also enables us to identify the optimal predictors to predict the balance: income, limit, cards, students in Table 6.
  
  STEP3. Using the result from step2, apply linear regression model on opitmal predictor variables to estimate the balance: income, limit, cards, student. Figure 1 shows the residual plot of OLS on test dataset.


  STEP4.  To identify the best OLS model. we are going to compare the Adjusted R square of both linear models. Model1, using selected predictor yields the `r round(summary(reg2)$adj.r.squared,5)`. Model2 using optimal predictor yields `r round(summary(reg3)$adj.r.squared,5)`. Therefore, the second model using optimal predictors is the best OLS model because it uses few variables but produces almost same Adjusted R square value as the first model as shown in Table 6.
  Lastly, using the best model we found, apply it to full data set to the "official coefficients", which is displayed at the end of analysis section.
  
###2) Ridge Regression(RR)

  
```{r echo = FALSE}
load("../../data/ridge-regression.RData")
```

  
  First, using ten-fold cross-validation, Ridge Regression selects the best model when $\lambda =\ `r par_ridge`$ and cross-validation plot is shown in Figure 2.
  
  After finding out the best model, applying the best model to our test set to compute the Test Mean Square Error, which is `r test_mse_ridge`, later use this figure to compare different method in the result section.
  
  Lastly, using the best model we found, apply it to full data set to the "official coefficients", which is displayed at the end of analysis section.
  

###3) Lasso Regression(LR)

```{r echo = FALSE}
load("../../data/lasso-regression.RData")
```

  
  Lasso Regression does exactly same procedure as Ridge Regression. It selecst the best model when  $\lambda =\ `r par_lasso`$ and cross-validation plot is shown in Figure 3.
  
  After finding out the best model, applying the best model to our test set to compute the Test Mean Square Error, which is `r test_mse_lasso`, later use this figure to compare different method in the result section.
  
  Lastly, using the best model we found, apply it to full data set to the "official coefficients", which is displayed at the end of analysis section.
  
###4) Principle Components Regression(PCR)

```{r echo = FALSE}
load("../../data/pcr-regression.RData")
```

  Principle Component Regression selecst the best model when  $M =\ `r par_pcr`$ where M represents the compression, reduced dimension and cross-validation plot is shown in Figure 4. 
  
  After finding out the best model, applying the best model to our test set to compute the Test Mean Square Error, which is `r test_mse_pcr`, later use this figure to compare different methods in the result section.
  
  Lastly, using the best model we found, apply it to full data set to the "official coefficients", which is displayed at the end of analysis section.
  

###5) Partial Least Squares Regression(PLSR)

```{r echo = FALSE}
load("../../data/plsr-regression.RData")
```

  Principle Component Regression selecst the best model when  $M =\ `r par_plsr`$ where M represents the compression, reduced dimension and cross-validation plot is shown in Figure 5. 
  
  After finding out the best model, applying the best model to our test set to compute the Test Mean Square Error, which is `r test_mse_plsr`, later use this figure to compare different methods in the result section.
  
  Lastly, using the best model we found, apply it to full data set to the "official coefficients", which is displayed at the end of analysis section.



```{r echo = FALSE, results='asis'}
load( "../../data/anova-qual.RData")

x = xtable(anova_ethnicity, digits=2, caption = "Anova analysis of ethnicity and Balance")

print(x, comment=FALSE, type="latex")

x = xtable(anova_gender, digits=2, caption = "Anova analysis of gender and Balance")
print(x, comment=FALSE, type="latex")

x = xtable(anova_marriage, digits=2, caption = "Anova analysis of marriage and Balance")
print(x, comment=FALSE, type="latex")

x = xtable(anova_student, digits=2, caption = "Anova analysis of student and Balance")
print(x, comment=FALSE, type="latex")

```


```{r echo = FALSE, results='asis'}
x = xtable(reg2, digits=4, "MODEL1 using selected predictor variables: income, limit, rating, cards, ages, eduction, student")
print(x, comment=FALSE, type='latex')

```
  
```{r echo = FALSE, results='asis'}
x = xtable(reg3, digits=4, "MODEL2 using Optimal predictor variables: income, limitm cards, student")
print(x, comment=FALSE, type='latex')

```

```{r echo = FALSE, results='asis'}
  k = data.frame(Adjusted.R.Square= c("Model1","Model2"),
                 Value = c(summary(reg2)$adj.r.squared,summary(reg3)$adj.r.squared))
  x=xtable(k,digits=4, captions = "Adjusted R Square of Model1 and Model2")
  print(x, comment=FALSE, type = 'latex', include.rownames =FALSE)

```
  
  
  
  
```{r echo = FALSE, results='asis'}
  ols_cof = as.data.frame(t(as.data.frame(cof_ols)))
  ridge_cof = as.data.frame(t(as.data.frame(cof_ridge)))
  lasso_cof = as.data.frame(t(as.data.frame(cof_lasso)))
  pcr_cof = as.data.frame(cof_pcr)
  plsr_cof = as.data.frame(cof_plsr)
  
```


\begin{figure}[b]
  \begin{center}
    \caption{Residuals plot of OLS on the test dataset}
    \centering
      \includegraphics[width=4in]{../../images/ols-residuals.png}
  \end{center}
\end{figure}
  
  
\begin{figure}[!b]
  \begin{center}
    \caption{Plot of the cross-validtion errors of Ridge Regression on Lambda}
    \centering
      \includegraphics[width=4in]{../../images/ridge-cross-validation-errors.png}
  \end{center}
\end{figure}


\begin{figure}[!b]
  \begin{center}
    \caption{Plot of the cross-validtion errors of Lasso Regression on Lambda}
    \centering
      \includegraphics[width=4in]{../../images/lasso-cross-validation-errors.png}
  \end{center}
\end{figure}


\begin{figure}[!b]
  \begin{center}
    \caption{Plot of the cross-validtion errors of PCR Regression on M}
    \centering
      \includegraphics[width=4in]{../../images/pcr-cross-validation-errors.png}
  \end{center}
\end{figure}


\begin{figure}[!b]
  \begin{center}
    \caption{Plot of the cross-validtion errors of PLSR Regression on M}
    \centering
      \includegraphics[width=4in]{../../images/plsr-cross-validation-errors.png}
  \end{center}
\end{figure}