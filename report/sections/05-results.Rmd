---
output: pdf_document
---
```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(xtable)
load("../../data/ols-regression.RData")
load("../../data/ridge-regression.RData")
load("../../data/lasso-regression.RData")
load("../../data/pcr-regression.RData")
load("../../data/plsr-regression.RData")
```

#Results

  From analyzing each regression model, we were able to come up with the Test MSE value. By comparing them, we can determine which method produced the smallest TEST MSE value, and therefore the best regression model.

  Table of TEST MSE values of each regression is shown in Table 10.

```{r echo=FALSE, results='asis'}
  names <- c('OLS', 'Ridge', 'Lasso', 'PCR', 'PLSR')
  mse_values <- c(test_mse_ols, test_mse_ridge, test_mse_lasso, test_mse_pcr, test_mse_plsr)
```

  According to the Table 10, `r names[which(mse_values == min(mse_values))]` has the smallest Test MSE is `r min(mse_values)`. 
  
  Therefore, we can conclude that among all the regression methods we considered, `r names[which(mse_values == min(mse_values))]` yields the smallest TEST MSE and thus is the best model for our given dataset.
  
  
  Lastly, we are going to compare the each official coefficients of regression methods. By plotting trend lines of the official coefficients. We can easily see that limit has the most influential aspect on balance on all regression method. Trend line plot is shown in Figure 6
  



