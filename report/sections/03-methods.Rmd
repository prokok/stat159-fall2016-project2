#Method

  For the project, we will analyze the given data with five different approaches: OLS(Ordinary Least Squares), RR(ridge regression), LR(lasso regression), PCR(principle components regression) and PLSR(partial least squares regression).


###1) Ordinary Least Square Method(OLS)  


$$\\Y = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \beta_{3}X_{3} + ... + \beta_{p}X_{p}$$   
$$\\RSS = \sum_{i=1}^{n} (y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2$$


  Ordinary least squares(OLS) is estimataing parameters in a linear regression model by minimizing the sum of squares of the distance between observed reponses and predcited reponses. This approach is perfomed under certain assumptions: errors are finite, homoscedastic, uncorrelated, and normally distributed. Under these assumption OLS provides the minimum-variance and unbiased mean estimators. 
  
  
## Shrinkage Methods

  Shrinkage Methods starts with fitting a modeling with all p predictors. However, it forces the estimated coefficients to be close to zero or to be zero. Also, it requires the data to be standardized before applying the method. There are two methods of shrinkage methods: Ridge Regression Method, and Lasso Regression Method. Depending on the method, some of the cofficients forced to be estimated to be exactly zero, which is the case of Lasso Regression. Thus, shrinkage Method does variable selection.
  

###2) Ridge Regression(RR)

$$\sum_{i=1}^{n} (y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2 + \lambda\sum_{j=1}^{p}\beta_{j}^2 = RSS + \lambda\sum_{j=1}^{p}\beta_{j}^2$$

$$minimize_{\beta} \{\sum_{i=1}^{n} (y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2\}\quad subject\  to\quad \lambda\sum_{j=1}^{p}\beta_{j}^2 \leq s$$

$$where\ every\ value\ of\ \lambda\ there\ is\ a\ corresponding\ s$$
  
  
  Equations above are two different representations of Ridge Regression. The first equation explains the similarity between OLS regression and Ridge Regression. Ridge regression calculates the coefficients by minimizing $RSS + \lambda\sum_{j=1}^{p}\beta_{j}^2$. If $\lambda$, the tunning parameter, is equal to zero, the estimated coefficients will be same as those of OLS regression. 
  The advantage of Ridge regression over OLS regression is bias-variance trade-off. As $\lambda$ increases, the estimated coefficients forced to be close to zero, leading bias estimators and decreasing the variance. As $\lambda$ decreases, the estimated coefficients do not have to be close to zero, leading less - bias estimators and increasing the variance. 
  

###3) Lasso Regression(LR)

$$\sum_{i=1}^{n} (y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2 + \lambda\sum_{j=1}^{p}\left|\beta_{j}\right| = RSS + \lambda\sum_{j=1}^{p}\left|\beta_{j}\right|$$

$$minimize_{\beta} \{\sum_{i=1}^{n} (y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2\}\quad subject\  to\quad \lambda\sum_{j=1}^{p}\left|\beta_{j}\right| \leq s$$

$$where\ every\ value\ of\ \lambda\ there\ is\ a\ corresponding\ s$$

  Equations above are two different representations of Lasso Regression. The first equation explains the similarity between OLS regression, Ridge regression and Lasso regression. Lasso regression calculates the coefficients by minimizing $RSS + \lambda\sum_{j=1}^{p}\left|\beta_{j}\right|$. If $\lambda$, the tunning parameter, is equal to zero, the estimated coefficients will be same as those of OLS regression. 
 Lasso regression has the same advantage over OLS regression as Ridge regression does: bias-variance trade-off. However, as $\lambda$ increases, the Lasso regression forces esimates to be exactly equal to zero. Thus, Lasso shrinkage method performs the variable selection and a model generated by Lasso regression is easier to interpret than a model generated by Ridge regression. Lasso regression involves only a subset of the variables referred as sparse-model.
 
 
## Dimension Reduction Methods

$$\\Z_{m} = \sum_{j=1}^{p}\phi_{jm}X_{j}$$
$$\\y_{i} = \theta_{0} + \sum_{m=1}^{M}\theta_{m}z_{im} + \varepsilon_{i} \quad i\ = \ 1, ..., n$$
$$\sum_{m=1}^{M}\theta_{m}z_{im}\ =\ \sum_{m=1}^{M}\theta_{m}\sum_{j=1}^{p}\phi_{jm}x_{ij}\ =\ \sum_{j=1}^{p}\sum_{m=1}^{M}\theta_{m}\phi_{jm}x_{ij}\ =\ \sum_{j=1}^{p}\beta_{j}x_{ij}$$ 
$$where\quad \beta_{j}\  =\ \sum_{m=1}^{M}\theta_{m}\phi_{jm}$$

  Dimenstion reduction method is transforming the predictors,$\\Z_{m} = \sum_{j=1}^{p}\phi_{jm}X_{j}$ and fit a least square models using transformed variables, $\\y_{i} = \theta_{0} + \sum_{m=1}^{M}\theta_{m}z_{im} + \varepsilon_{i} \quad i\ = \ 1, ..., n$. This approach is used when there is a large number of predictor variables. By computing M different linear combinations, transforming the predictors, we can project p predictors into a M-dimensional subspace where M is less than or equal to p. Then M projections can be used as predictors to fit a linear model by ordinary least squares.
  If M is equal to p, there is no dimention reduction occurs, yielding the same result equivalent to performing ordinary least squares on the p predictors.
  There are two major dimension reduction methods: principal components regression and partial least squares regression. Those methods work in two steps. First step is transforming the predictors, $\\Z_{m} = \sum_{j=1}^{p}\phi_{jm}X_{j}$. Second steop is selecting $\phi_{jm}$.

###4) Principle Components Regression(PCR)
###5) Partial Least Squares Regression(PLSR)

